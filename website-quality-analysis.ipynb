{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bda52d7d-07c3-4c1a-ac3c-669e690030f5",
   "metadata": {},
   "source": [
    "# Website Quality Analysis\n",
    "\n",
    "Analyze website quality using data generated from Google Lighthouse, Pandas dataframes, and visualizations.\n",
    "\n",
    "## Steps\n",
    "\n",
    "1. Find all internal links on website\n",
    "1. Build dataframe of links\n",
    "1. Assess website quality scores on a per link basis, storing in dataframe\n",
    "1. Sort and display dataframe results\n",
    "1. Visualize results as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e68f52d-2efe-4229-8431-20ef7125963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "import colorama\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from plotly.colors import n_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f034b8-79bc-444a-96aa-fd7f5e8195b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_url = \"https://<your website here>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55153d3d-0ee0-49b1-8efe-9e9c6c00b073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lightouse json report audits don't include categories,\n",
    "# so we run a single csv report to gather audit id's per category\n",
    "target_url_str = \"{}\".format(target_url).replace(\"https://\", \"\").replace(\"/\", \"_\")\n",
    "command = \"lighthouse --no-update-notifier --no-enable-error-reporting --output=csv --output-path={}  --chrome-flags='--headless' {}\".format(\n",
    "    target_url_str, target_url\n",
    ")\n",
    "p = subprocess.Popen(\n",
    "    command,\n",
    "    shell=True,\n",
    ")\n",
    "p.communicate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa92e388-93c5-4b0c-a2ca-028d796a526e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gather categories of audit ids from csv\n",
    "df_cats = pd.read_csv(target_url_str)[[\"name\", \"category\"]]\n",
    "df_cats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a6118eb-bf38-49e9-b38b-a76ff9a4f3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scraping code courtesy of @x4nth055 from the following link:\n",
    "# https://github.com/x4nth055/pythoncode-tutorials/blob/master/web-scraping/link-extractor/link_extractor.py\n",
    "\n",
    "# init the colorama module\n",
    "colorama.init()\n",
    "GREEN = colorama.Fore.GREEN\n",
    "GRAY = colorama.Fore.LIGHTBLACK_EX\n",
    "RESET = colorama.Fore.RESET\n",
    "YELLOW = colorama.Fore.YELLOW\n",
    "\n",
    "\n",
    "def is_valid(url):\n",
    "    \"\"\"\n",
    "    Checks whether `url` is a valid URL.\n",
    "    \"\"\"\n",
    "    parsed = urlparse(url)\n",
    "    return bool(parsed.netloc) and bool(parsed.scheme)\n",
    "\n",
    "\n",
    "def get_all_website_links(url):\n",
    "    \"\"\"\n",
    "    Returns all URLs that is found on `url` in which it belongs to the same website\n",
    "    \"\"\"\n",
    "    # all URLs of `url`\n",
    "    urls = set()\n",
    "    # domain name of the URL without the protocol\n",
    "    domain_name = urlparse(url).netloc\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36\"\n",
    "    }\n",
    "    soup = BeautifulSoup(\n",
    "        requests.get(url, headers=headers, verify=False).content, \"html.parser\"\n",
    "    )\n",
    "\n",
    "    for a_tag in soup.findAll(\"a\"):\n",
    "\n",
    "        href = a_tag.attrs.get(\"href\")\n",
    "        if href == \"\" or href is None:\n",
    "            # href empty tag\n",
    "            continue\n",
    "\n",
    "        # join the URL if it's relative (not absolute link)\n",
    "        href = urljoin(url, href)\n",
    "\n",
    "        parsed_href = urlparse(href)\n",
    "        # remove URL GET parameters, URL fragments, etc.\n",
    "        href = parsed_href.scheme + \"://\" + parsed_href.netloc + parsed_href.path\n",
    "\n",
    "        if not is_valid(href):\n",
    "            # not a valid URL\n",
    "            continue\n",
    "        if href in internal_urls:\n",
    "            # already in the set\n",
    "            continue\n",
    "        if domain_name not in href:\n",
    "            # external link\n",
    "            if href not in external_urls:\n",
    "                # print(f\"{GRAY}[!] External link: {href}{RESET}\")\n",
    "                external_urls.add(href)\n",
    "            continue\n",
    "        # print(f\"{GREEN}[*] Internal link: {href}{RESET}\")\n",
    "        urls.add(href)\n",
    "        internal_urls.add(href)\n",
    "\n",
    "    return urls\n",
    "\n",
    "\n",
    "# number of urls visited so far will be stored here\n",
    "total_urls_visited = 0\n",
    "\n",
    "\n",
    "def crawl(url, max_urls=1000):\n",
    "    \"\"\"\n",
    "    Crawls a web page and extracts all links.\n",
    "    You'll find all links in `external_urls` and `internal_urls` global set variables.\n",
    "    params:\n",
    "        max_urls (int): number of max urls to crawl, default is 30.\n",
    "    \"\"\"\n",
    "    global total_urls_visited\n",
    "    total_urls_visited += 1\n",
    "    # print(f\"{YELLOW}[*] Crawling: {url}{RESET}\")\n",
    "    links = get_all_website_links(url)\n",
    "    for link in links:\n",
    "        if total_urls_visited > max_urls:\n",
    "            break\n",
    "        crawl(link, max_urls=max_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95414e9e-6f95-413b-a456-9568bcb5761b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the set of links (unique links)\n",
    "internal_urls = set()\n",
    "external_urls = set()\n",
    "\n",
    "crawl(target_url)\n",
    "print(\"[+] Total Internal links:\", len(internal_urls))\n",
    "print(\"[+] Total External links:\", len(external_urls))\n",
    "print(\"[+] Total URLs:\", len(external_urls) + len(internal_urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1131eec0-bfb3-4225-a791-7b4a31a24a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(internal_urls)\n",
    "df = df.rename(columns={0: \"URL\"})\n",
    "df = df[\n",
    "    (~df[\"URL\"].str.contains(\"tel:\"))\n",
    "    & (~df[\"URL\"].str.contains(\"mailto:\"))\n",
    "    & (~df[\"URL\"].str.contains(\"http://\"))\n",
    "]\n",
    "df[\"URL\"] = df[\"URL\"].str.rstrip(\"/\")\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808fe09e-4705-48c5-a0a6-b70ebf4c3dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lighthouse_audit(target_url: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    takes url generate lighthouse report from system\n",
    "    returns dataframe of result\n",
    "    \"\"\"\n",
    "\n",
    "    # prepare command\n",
    "    command = \"lighthouse --no-update-notifier --no-enable-error-reporting --output=json  --chrome-flags='--headless' {}\".format(\n",
    "        target_url\n",
    "    )\n",
    "\n",
    "    # run command from system\n",
    "    p = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n",
    "\n",
    "    # transform stdout str results to json dict\n",
    "    json_result = json.loads(p.communicate()[0])\n",
    "\n",
    "    # send audit results to pd.dataframe\n",
    "    df = pd.DataFrame(json_result[\"audits\"]).T\n",
    "\n",
    "    # add url to result audits\n",
    "    df[\"url\"] = target_url\n",
    "\n",
    "    # reorder df to show url as first column\n",
    "    df = df[[\"url\"] + [col for col in df.columns if col != \"url\"]]\n",
    "\n",
    "    # reset the index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # set the categories per audit id\n",
    "    df[\"category\"] = df[\"id\"]\n",
    "    df[\"category\"] = df[\"category\"].map(\n",
    "        dict(zip(df_cats[\"name\"].values.tolist(), df_cats[\"category\"].values.tolist()))\n",
    "    )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d595aa41-45ef-4b0e-876d-641b799f90b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "\n",
    "# go through each url and gather audit results\n",
    "for target_url in df[\"URL\"].tolist():\n",
    "    df_list.append(get_lighthouse_audit(target_url))\n",
    "\n",
    "print(len(df_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb72b1-2355-4964-ad55-1ffb584a32dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# join the results as one dataframe, or relable the first element as the same\n",
    "if len(df_list) > 1:\n",
    "    results = pd.concat(df_list)\n",
    "else:\n",
    "    results = df_list[0]\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15568278-2ca6-4692-a8b0-be6bb16ef609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter results to only those we'll use to report on.\n",
    "results_filtered = results[\n",
    "    (~results[\"category\"].isna())\n",
    "    & (~(results[\"scoreDisplayMode\"] == \"notApplicable\"))\n",
    "    & (~(results[\"scoreDisplayMode\"] == \"manual\"))\n",
    "    & (~(results[\"scoreDisplayMode\"] == \"informative\"))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5bdc26-5932-4ef8-a95e-3bd27c411f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the aggregated scores of each category by url\n",
    "aggregate_scores = (\n",
    "    results_filtered.groupby([\"url\", \"category\"])[\"score\"].sum()\n",
    "    / results_filtered.groupby([\"url\", \"category\"])[\"id\"].count()\n",
    ")\n",
    "aggregate_scores.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e373470-1415-4d4c-a5db-d773f0935038",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show mean of all urls\n",
    "average_score = aggregate_scores.unstack(level=-1).mean().to_frame().T\n",
    "\n",
    "# round scores as integers\n",
    "average_score = (average_score.round(decimals=2) * 100).astype(\"int\")\n",
    "\n",
    "average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e7d62a-af5f-4fb6-9458-ba743427cee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = n_colors(\"rgb(250, 0, 50)\", \"rgb(100, 200, 0)\", 101, colortype=\"rgb\")\n",
    "\n",
    "\n",
    "fig = go.Figure(\n",
    "    data=[\n",
    "        go.Table(\n",
    "            header=dict(\n",
    "                values=list(average_score.columns),\n",
    "                fill_color=\"paleturquoise\",\n",
    "                align=\"center\",\n",
    "                font=dict(color=\"black\", size=11),\n",
    "            ),\n",
    "            cells=dict(\n",
    "                values=[\n",
    "                    average_score[\"Accessibility\"],\n",
    "                    average_score[\"Best Practices\"],\n",
    "                    average_score[\"Performance\"],\n",
    "                    average_score[\"Progressive Web App\"],\n",
    "                    average_score[\"SEO\"],\n",
    "                ],\n",
    "                line_color=[\n",
    "                    np.array(colors)[average_score[\"Accessibility\"]],\n",
    "                    np.array(colors)[average_score[\"Best Practices\"]],\n",
    "                    np.array(colors)[average_score[\"Performance\"]],\n",
    "                    np.array(colors)[average_score[\"Progressive Web App\"]],\n",
    "                    np.array(colors)[average_score[\"SEO\"]],\n",
    "                ],\n",
    "                fill_color=[\n",
    "                    np.array(colors)[average_score[\"Accessibility\"]],\n",
    "                    np.array(colors)[average_score[\"Best Practices\"]],\n",
    "                    np.array(colors)[average_score[\"Performance\"]],\n",
    "                    np.array(colors)[average_score[\"Progressive Web App\"]],\n",
    "                    np.array(colors)[average_score[\"SEO\"]],\n",
    "                ],\n",
    "                align=\"center\",\n",
    "                font=dict(color=\"white\", size=11),\n",
    "            ),\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "fig.write_image(\"lighthouse_overall_average_score.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28dfa29-588b-4a10-ac5c-a1fde3738731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target accessibility category aggregate scores\n",
    "aggregate_scores.unstack().sort_values(\"Accessibility\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d273fe-6d2c-493b-9503-9c6635f670b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target accessibility category aggregate scores as csv\n",
    "aggregate_scores.unstack().sort_values(\"Accessibility\", ascending=False).to_csv(\n",
    "    \"lighthouse_category_scores_by_url.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab84138-6ae8-4a43-a937-ab5be51012a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target all category individual scores as csv\n",
    "results_filtered.sort_values([\"url\", \"category\"]).to_csv(\n",
    "    \"lighthouse_scores_by_url.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2d67a5-6489-4a9b-92d0-1ce6b6ce5685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target accessibility category individual scores as csv\n",
    "results_filtered[\n",
    "    (results_filtered[\"category\"] == \"Accessibility\") & (results_filtered[\"score\"] != 1)\n",
    "].sort_values([\"url\", \"category\"]).to_csv(\n",
    "    \"lighthouse_accessibility_low_scores_by_url.csv\", index=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
